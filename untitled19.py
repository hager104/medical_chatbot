# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZRnAo_MN5qi2uKtAdfgsANQPE2CG8Hc7

#installing lib
"""

!pip install -U langchain-community

!pip install ctransformers

!pip install Pinecone

"""#import lib"""

from langchain import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import CTransformers
from langchain.vectorstores import Pinecone
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
import pandas as panda

"""#create API_KEY AND API_ENV"""

PINECONE_API_key="pcsk_29YpbD_SHWVCkHpPQMnuto42uEqyduYCM2D3SeUNV5guDWeMNrj71rZMASqzNr26EvisMJ"
PINECONE_API_ENV="us-east-1"

"""#data set"""

import pandas as pd
df = pd.read_csv("/content/Medical_Intelligence_Dataset (1).csv")
# Display the first 5 rows of the dataset
print(df.head())
# Display some basic information about the dataset
print(df.info())

"""#Clean the data set and Create ,implementation chunks"""

import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. clean the data
df = df.dropna(subset=['input'])
df = df[df['input'].str.strip() != '']

# processed function 'text_split'
def text_split(text):
    """division the data by RecursiveCharacterTextSplitter."""
    text_splitter = RecursiveCharacterTextSplitter(
       #Portion size
        chunk_size=500,
       #part overlap
        chunk_overlap=20

    )
    if isinstance(text, str) and pd.notna(text) and len(text) > 0:  # التحقق من أن النص ليس فارغًا
        return text_splitter.split_text(text)
    else:
        return [] # Added indentation here

#  implementation and create chunks
df['chunks'] = df['input'].apply(text_split)
df_chunked = df.explode('chunks', ignore_index=True)

print(df_chunked.head())

print(f"length of chunks: {len(df_chunked)}")

"""# Downloading hugging face embeddings model"""

#downlioading hugging face embeddinds model
def download_hugging_face_embeddings():
  embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
  return embeddings

embeddings=download_hugging_face_embeddings()

embeddings

query_result=embeddings.embed_query("hello world")
print("length=",len(query_result))

query_result

"""#connect PineCone with the code   """

import os
import pinecone
from langchain.vectorstores import Pinecone as LangchainPinecone

# Set Pinecone API key and environment as environment variables
os.environ["PINECONE_API_KEY"] = "pcsk_29YpbD_SHWVCkHpPQMnuto42uEqyduYCM2D3SeUNV5guDWeMNrj71rZMASqzNr26EvisMJ"  # Replace with your actual API key
os.environ["PINECONE_ENVIRONMENT"] = "us-east-1"

# Initialize Pinecone by creating an instance of the Pinecone class
pc = pinecone.Pinecone(
    api_key=os.environ["PINECONE_API_KEY"],
    environment=os.environ["PINECONE_ENVIRONMENT"]
)

# Define index name
index_name = "medicalchatbot"

# Create embeddings and store in Pinecone
docsearch = LangchainPinecone.from_texts(
    df_chunked['chunks'].tolist(),
    embeddings,
    index_name=index_name
)

docsearch=LangchainPinecone.from_existing_index(index_name,embeddings)
query=" What is (are) atelosteogenesis type 1 ?"
docs=docsearch.similarity_search(query,k=3)
print("result",docs)

prompt_tamplate='''Use the following pieces of information to answer the user's question.
if you don't know the answer, just say that you don't know, don't try to make up an answer.
Context:{context}
Question:{question}

only return the helpful answer below and nothing else.
Helpful answer
 '''

PROMPT=PromptTemplate(template=prompt_tamplate,input_variables=['context','question'])
chain_type_kwargs={"prompt":PROMPT}

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

"""#llm(llama) model"""

!pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

llm = CTransformers(
    model="TheBloke/Llama-2-7B-Chat-GGML",  # Updated model path
    model_type="llama",
    config={
        'max_new_tokens': 512,
        'temperature': 0.8
    }
)

qa=RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={"k":2}),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs,
)

while True:
  user_input = input("input prompt:")
  if user_input.lower() == "exit":
    break
  result = qa({"query": user_input})
  print("response:", result['result'])

